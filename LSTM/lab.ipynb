{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ модели LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предварительная работа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nerus in /home/panterrich/.local/lib/python3.8/site-packages (1.7.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nerus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14579/475690356.py:19: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook       import tqdm\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from copy                    import deepcopy\n",
    "from matplotlib.image        import imread\n",
    "from mpl_toolkits            import mplot3d\n",
    "from matplotlib              import gridspec\n",
    "from nerus                   import load_nerus\n",
    "from skimage.segmentation    import mark_boundaries\n",
    "from sklearn.metrics         import classification_report\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from torch.utils             import data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision             import datasets, transforms\n",
    "from tqdm.autonotebook       import tqdm\n",
    "from PIL                     import Image\n",
    "from urllib.request          import urlopen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Установка вычислительного устройства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(dataset, word2idx, tag2idx, batch_size=64, shuffle=True):\n",
    "    X, Y = dataset[0], dataset[1]\n",
    "\n",
    "    PAD = word2idx['<PAD>']\n",
    "    n_samples = len(X)\n",
    "\n",
    "# генерим список индексов\n",
    "    list_of_indexes = np.linspace(\n",
    "        0, n_samples - 1, n_samples, dtype=np.int64)\n",
    "    List_X = []\n",
    "    List_Y = []\n",
    "\n",
    "# если нужно перемешать, то перемешиваем\n",
    "    if shuffle:\n",
    "        np.random.shuffle(list_of_indexes)\n",
    "\n",
    "# сгенерируем список индексов, по этим индексам,\n",
    "# сделаем новый перемешаный спиисок токенов и тэгов\n",
    "    for indx in list_of_indexes:\n",
    "        List_X.append(X[indx])\n",
    "        List_Y.append(Y[indx])\n",
    "\n",
    "    n_batches = n_samples//batch_size\n",
    "    if n_samples%batch_size != 0:\n",
    "        n_batches+=1\n",
    "\n",
    "    # For each k yield pair x and y\n",
    "    for k in range(n_batches):\n",
    "# указываем текущии размер батча\n",
    "        this_batch_size = batch_size\n",
    "\n",
    "# если мы выдаем последний батч, то его нужно обрезать\n",
    "        if k == n_batches - 1:\n",
    "            if n_samples%batch_size > 0:\n",
    "                this_batch_size = n_samples%batch_size\n",
    "\n",
    "        This_X = List_X[k*batch_size:k*batch_size + this_batch_size]\n",
    "        This_Y = List_Y[k*batch_size:k*batch_size + this_batch_size]\n",
    "\n",
    "        This_X_line = [\n",
    "                       [word2idx.get('<START>', 0)] \\\n",
    "                       + [word2idx.get(word, 1) for word in sent] \\\n",
    "                       + [word2idx.get('<FINISH>', 0)]\\\n",
    "                       for sent in This_X]\n",
    "        This_Y_line = [\n",
    "                       [tag2idx.get('<START>', 0)]\\\n",
    "                       + [tag2idx.get(tag, 1) for tag in sent]\\\n",
    "                       + [tag2idx.get('<FINISH>', 0)]\\\n",
    "                       for sent in This_Y]\n",
    "\n",
    "        List_of_length_x = [len(sent) for sent in This_X_line]\n",
    "        length_of_sentence_x = max(List_of_length_x)\n",
    "        List_of_length_y = [len(sent) for sent in This_Y_line]\n",
    "        length_of_sentence_y = max(List_of_length_y)\n",
    "\n",
    "        x_arr = np.ones(shape=[this_batch_size, length_of_sentence_x])*PAD\n",
    "        y_arr = np.ones(shape=[this_batch_size, length_of_sentence_y])*PAD\n",
    "\n",
    "        for i in range(this_batch_size):\n",
    "            x_arr[i, :len(This_X_line[i])] = This_X_line[i]\n",
    "            y_arr[i, :len(This_Y_line[i])] = This_Y_line[i]\n",
    "\n",
    "        x = torch.LongTensor(x_arr)\n",
    "        y = torch.LongTensor(y_arr)\n",
    "        lengths = torch.LongTensor(List_of_length_x)\n",
    "\n",
    "        yield x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(model, batch_of_x, batch_of_y, optimizer, loss_function):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "\n",
    "    output = model(batch_of_x.to(model.device)).transpose(1, 2)\n",
    "\n",
    "    loss = loss_function(output, batch_of_y.to(model.device))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.cpu().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_generator,\n",
    "                model,\n",
    "                loss_function,\n",
    "                optimizer,\n",
    "                callback = None):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    total = 0\n",
    "    for it, (batch_of_x, batch_of_y) in enumerate(train_generator):\n",
    "        batch_loss = train_on_batch(model,\n",
    "                                    batch_of_x,\n",
    "                                    batch_of_y,\n",
    "                                    optimizer,\n",
    "                                    loss_function)\n",
    "\n",
    "        train_generator.set_postfix({'train batch loss': batch_loss})\n",
    "\n",
    "        if callback is not None:\n",
    "            callback(model, batch_loss)\n",
    "\n",
    "        epoch_loss += batch_loss*len(batch_of_x)\n",
    "        total += len(batch_of_x)\n",
    "\n",
    "    return epoch_loss/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(count_of_epoch,\n",
    "            batch_size,\n",
    "            model,\n",
    "            dataset,\n",
    "            word2idx,\n",
    "            tag2idx,\n",
    "            loss_function,\n",
    "            optimizer,\n",
    "            callback):\n",
    "    iterations = tqdm(range(count_of_epoch), desc='epoch')\n",
    "    iterations.set_postfix({'train epoch loss': np.nan})\n",
    "\n",
    "    optima = optimizer\n",
    "\n",
    "    n_samples = len(dataset[0])\n",
    "    number_of_batch = n_samples//batch_size + (n_samples%batch_size>0)\n",
    "\n",
    "    for it in iterations:\n",
    "        generator = tqdm(\n",
    "            data_loader(dataset, word2idx, tag2idx, batch_size),\n",
    "            leave=False, total=number_of_batch)\n",
    "\n",
    "        epoch_loss = train_epoch(\n",
    "            train_generator = generator,\n",
    "            model = model,\n",
    "            loss_function = loss_function,\n",
    "            optimizer = optima,\n",
    "            callback=callback)\n",
    "\n",
    "        iterations.set_postfix({'train epoch loss': epoch_loss})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отслеживание обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 15463), started 0:00:22 ago. (Use '!kill 15463' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9120e75bfc9eb677\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9120e75bfc9eb677\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir experiment/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class callback():\n",
    "    def __init__(self, writer, dataset, loss_function,\n",
    "                 word2idx, tag2idx, delimeter = 100, batch_size=64):\n",
    "        self.step = 0\n",
    "        self.writer = writer\n",
    "        self.delimeter = delimeter\n",
    "        self.loss_function = loss_function\n",
    "        self.word2idx = word2idx\n",
    "        self.tag2idx = tag2idx\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def forward(self, model, loss):\n",
    "        self.step += 1\n",
    "        self.writer.add_scalar('LOSS/train', loss, self.step)\n",
    "\n",
    "        if self.step % self.delimeter == 0:\n",
    "\n",
    "            batch_generator = data_loader(dataset = self.dataset,\n",
    "                                          batch_size = self.batch_size,\n",
    "                                          word2idx = self.word2idx,\n",
    "                                          tag2idx = self.tag2idx)\n",
    "            pred = []\n",
    "            real = []\n",
    "            test_loss = 0\n",
    "            for it, (x_batch, y_batch) in enumerate(batch_generator):\n",
    "                x_batch = x_batch.to(model.device)\n",
    "                y_batch = y_batch.to(model.device)\n",
    "\n",
    "                output = model(x_batch)\n",
    "\n",
    "                pred.extend(torch.argmax(output, dim=2).cpu().numpy().flatten().tolist())\n",
    "                real.extend(y_batch.cpu().numpy().flatten().tolist())\n",
    "\n",
    "                output = output.transpose(1, 2)\n",
    "                test_loss += self.loss_function(output, y_batch).cpu().item()*len(x_batch)\n",
    "\n",
    "            test_loss /= len(self.dataset[0])\n",
    "\n",
    "            self.writer.add_scalar('LOSS/test', test_loss, self.step)\n",
    "\n",
    "            self.writer.add_text('REPORT/test', str(classification_report(real, pred)), self.step)\n",
    "\n",
    "    def __call__(self, model, loss):\n",
    "        return self.forward(model, loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель нейросети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(torch.nn.Module):\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_dim,\n",
    "                 tagger_dim,\n",
    "                 is_batch_norm = False,\n",
    "                 drop_out = 0,\n",
    "                 emb_dim = 10,\n",
    "                 hidden_dim = 10,\n",
    "                 num_layers = 3):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "\n",
    "        self.drop_out = drop_out\n",
    "        self.is_batch_norm = is_batch_norm\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(vocab_dim, emb_dim)\n",
    "\n",
    "        self.batch_norm_inp = torch.nn.BatchNorm1d(emb_dim)\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(emb_dim, hidden_dim, num_layers,\n",
    "                                  dropout = drop_out, batch_first=True)\n",
    "\n",
    "        self.linear = torch.nn.Linear(hidden_dim, tagger_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = self.embedding(input)\n",
    "\n",
    "        if self.is_batch_norm:\n",
    "            input = input.transpose(1, 2)\n",
    "            norm = self.batch_norm_inp(input)\n",
    "            norm = norm.transpose(1, 2)\n",
    "            d, (_, _) = self.lstm(norm)\n",
    "        else:\n",
    "            d, (_, _) = self.lstm(input)\n",
    "\n",
    "        return self.linear(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выборка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_nerus('nerus_lenta.conllu.gz')\n",
    "\n",
    "train_size = 4200\n",
    "test_size = 800\n",
    "\n",
    "train_tokens = []\n",
    "train_tags = []\n",
    "\n",
    "test_tokens = []\n",
    "test_tags = []\n",
    "\n",
    "for _ in range(train_size):\n",
    "    doc = next(docs)\n",
    "    for sent in doc.sents:\n",
    "        train_tokens.append([])\n",
    "        train_tags.append([])\n",
    "        for nerus_token in sent.tokens:\n",
    "            train_tokens[-1].append(nerus_token.text)\n",
    "            train_tags[-1].append(nerus_token.tag)\n",
    "\n",
    "\n",
    "for _ in range(test_size):\n",
    "    doc = next(docs)\n",
    "    for sent in doc.sents:\n",
    "        test_tokens.append([])\n",
    "        test_tags.append([])\n",
    "        for nerus_token in sent.tokens:\n",
    "            test_tokens[-1].append(nerus_token.text)\n",
    "            test_tags[-1].append(nerus_token.tag)\n",
    "\n",
    "dataset_train = [train_tokens, train_tags]\n",
    "dataset_test = [test_tokens, test_tags]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {'<PAD>': 0, '<UNK>': 1, '<START>': 2, '<FINISH>': 3}\n",
    "idx2word = {0: '<PAD>', 1: '<UNK>', 2: '<START>', 3: '<FINISH>'}\n",
    "\n",
    "tag2idx = {'<PAD>': 0, '<UNK>': 1, '<START>': 2, '<FINISH>': 3}\n",
    "idx2tag = {0: '<PAD>', 1: '<UNK>', 2: '<START>', 3: '<FINISH>'}\n",
    "\n",
    "tokens = [token for sent in train_tokens for token in sent]\n",
    "tags = [tag for sent in train_tags for tag in sent]\n",
    "\n",
    "for item in list(set(tokens)):\n",
    "    word2idx[item] = len(word2idx)\n",
    "    idx2word[word2idx[item]] = item\n",
    "\n",
    "for item in list(set(tags)):\n",
    "    tag2idx[item] = len(tag2idx)\n",
    "    idx2tag[tag2idx[item]] = item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e03b4b9d8b4e919d95552bcd56a3c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid = ParameterGrid({'hidden_dim': [100, 200],\n",
    "                      'emb_dim': [100, 200],\n",
    "                      'num_layers': [2, 3],\n",
    "                      'is_batch_norm': [False, True],\n",
    "                      'drop_out': [0, 0.5],\n",
    "                      'vocab_dim': [len(word2idx)],\n",
    "                      'tagger_dim': [len(tag2idx)]})\n",
    "\n",
    "for item in tqdm(grid):\n",
    "    model = LSTMTagger(**item)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    name = 'experiment/hidden{}_emb{}_layers{}_vocab{}'.format(\n",
    "            item['hidden_dim'], item['emb_dim'], item['num_layers'], item['vocab_dim'])\n",
    "\n",
    "    if item['is_batch_norm']:\n",
    "        name += '_norm'\n",
    "\n",
    "    if item['drop_out'] > 0:\n",
    "        name += 'drop{}'.format(item['drop_out'])\n",
    "\n",
    "    writer = SummaryWriter(log_dir = name)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_function = torch.nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])\n",
    "\n",
    "    call = callback(writer, dataset_test, loss_function, word2idx, tag2idx, delimeter = 10)\n",
    "\n",
    "    trainer(count_of_epoch = 5,\n",
    "            batch_size = 64,\n",
    "            model = model,\n",
    "            dataset = dataset_train,\n",
    "            word2idx = word2idx,\n",
    "            tag2idx = tag2idx,\n",
    "            loss_function = loss_function,\n",
    "            optimizer = optimizer,\n",
    "            callback = call)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной работе была изучена работа RNN сетей, на примере LSTM и датасета Nerus.\n",
    "\n",
    "Размер словаря очень сильно опредяет обучающую способность модели, поэтому его размер не менялся, так как результат был предсказуемым.\n",
    "\n",
    "Наихудшие результаты показали следующие модели: hidden100_emb100_layers2_vocab91394, hidden200_emb200_layers2_vocab91394, hidden100_emb200_layers2_vocab91394. Заметно, что модель стала переобучаться.\n",
    "\n",
    "Лучший результат показала следующая модель: hidden200_emb200_layers2_vocab91394_norm.\n",
    "\n",
    "Наличие нормировки сказалось не только на быстроту сходимости модели, но и на отсутствии переобучения. Выбрасывания нейронов также приводило к улучшению результата. Для более лучшего результата размерность скрытых слоев должна быть не больше embedding размерности. Количество слоёв LSTM почти не отражалось на результате."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
