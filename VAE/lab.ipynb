{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ модели вариационного автокодировщика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предварительная работа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7655/139082540.py:22: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook       import tqdm\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import math\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from copy                    import deepcopy\n",
    "from matplotlib.image        import imread\n",
    "from mpl_toolkits            import mplot3d\n",
    "from matplotlib              import gridspec\n",
    "from nerus                   import load_nerus\n",
    "from nltk.tokenize           import RegexpTokenizer\n",
    "from skimage.segmentation    import mark_boundaries\n",
    "from sklearn.metrics         import classification_report\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from torch.utils             import data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision             import datasets, transforms\n",
    "from tqdm.autonotebook       import tqdm\n",
    "from PIL                     import Image\n",
    "from urllib.request          import urlopen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Установка вычислительного устройства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(model, x_batch, y_batch, optimizer, loss_function):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = model.loss(x_batch.to(model.device), y_batch.to(model.device))\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    return loss.cpu().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_generator,\n",
    "                model,\n",
    "                loss_function,\n",
    "                optimizer,\n",
    "                callback = None):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    total = 0\n",
    "    for it, (batch_of_x, batch_of_y) in enumerate(train_generator):\n",
    "        batch_loss = train_on_batch(model,\n",
    "                                    batch_of_x,\n",
    "                                    batch_of_y,\n",
    "                                    optimizer,\n",
    "                                    loss_function)\n",
    "        if callback is not None:\n",
    "            with torch.no_grad():\n",
    "                callback(model, batch_loss)\n",
    "\n",
    "        epoch_loss += batch_loss*len(batch_of_x)\n",
    "        total += len(batch_of_x)\n",
    "\n",
    "    return epoch_loss/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(count_of_epoch,\n",
    "            batch_size,\n",
    "            model,\n",
    "            dataset,\n",
    "            loss_function,\n",
    "            optimizer,\n",
    "            lr = 0.001,\n",
    "            callback = None):\n",
    "    iterations = tqdm(range(count_of_epoch), desc='epoch')\n",
    "    iterations.set_postfix({'train epoch loss': np.nan})\n",
    "\n",
    "    n_samples = len(dataset)\n",
    "    number_of_batch = n_samples//batch_size + (n_samples%batch_size>0)\n",
    "\n",
    "    for it in iterations:\n",
    "        batch_generator = tqdm(\n",
    "            torch.utils.data.DataLoader(dataset = dataset, batch_size = batch_size, shuffle=True),\n",
    "            leave=False, total=number_of_batch)\n",
    "\n",
    "        epoch_loss = train_epoch(\n",
    "            train_generator = batch_generator,\n",
    "            model = model,\n",
    "            loss_function = loss_function,\n",
    "            optimizer = optimizer,\n",
    "            callback=callback)\n",
    "\n",
    "        iterations.set_postfix({'train epoch loss': epoch_loss})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отслеживание обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 18029), started 0:00:02 ago. (Use '!kill 18029' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8015eb1083ea55b2\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8015eb1083ea55b2\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir experiment/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class callback():\n",
    "    def __init__(self, writer, dataset,\n",
    "                 loss_function, delimeter = 100, batch_size=64):\n",
    "        self.step = 0\n",
    "        self.writer = writer\n",
    "        self.delimeter = delimeter\n",
    "        self.loss_function = loss_function\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def forward(self, model, loss):\n",
    "        self.step += 1\n",
    "        self.writer.add_scalar('LOSS/train', loss, self.step)\n",
    "\n",
    "        if self.step % self.delimeter == 0:\n",
    "\n",
    "            batch_generator = torch.utils.data.DataLoader(dataset = self.dataset,\n",
    "                                                          batch_size = self.batch_size)\n",
    "\n",
    "            test_loss = 0\n",
    "            model.eval()\n",
    "            for it, (x_batch, y_batch) in enumerate(batch_generator):\n",
    "                x_batch = x_batch.to(model.device)\n",
    "\n",
    "                test_loss += model.loss(x_batch, y_batch).cpu().item()*len(x_batch)\n",
    "\n",
    "            test_loss /= len(self.dataset)\n",
    "\n",
    "            self.writer.add_scalar('LOSS/test', test_loss, self.step)\n",
    "\n",
    "    def __call__(self, model, loss):\n",
    "        return self.forward(model, loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель вариационного автокодировщика "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(torch.nn.Module):\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def __init__(self, latent_dim, input_dim, num_layers, hidden_dim=200):\n",
    "        \"\"\"\n",
    "        Standart model of VAE with ELBO optimization.\n",
    "        Args:\n",
    "            latent_dim: int - the dimension of latent space.\n",
    "            input_dim: int - the dimension of input space.\n",
    "            hidden_dim: int - the size of hidden_dim neural layer.\n",
    "        Returns:\n",
    "            None\n",
    "        Example:\n",
    "            >>> model = VAE(2, 10)\n",
    "        \"\"\"\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.proposal_z = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.input_dim, hidden_dim),\n",
    "            torch.nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.proposal_mu = torch.nn.Linear(hidden_dim, self.latent_dim)\n",
    "        self.proposal_sigma = torch.nn.Linear(hidden_dim, self.latent_dim)\n",
    "\n",
    "        self.generative_layear = torch.nn.Sequential(\n",
    "            torch.nn.BatchNorm1d(self.latent_dim),\n",
    "            torch.nn.Linear(self.latent_dim, num_layers * hidden_dim),\n",
    "            torch.nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        if num_layers > 1:\n",
    "            self.middle_layer = torch.nn.Sequential()\n",
    "\n",
    "            for i in range(num_layers - 1):\n",
    "                self.middle_layer.add_module(\"Norm\" + str(i), torch.nn.BatchNorm1d((num_layers - i) * hidden_dim))\n",
    "                self.middle_layer.add_module(\"Linear\" + str(i),\n",
    "                                             torch.nn.Linear((num_layers - i    ) * hidden_dim,\n",
    "                                                             (num_layers - i - 1) * hidden_dim))\n",
    "                self.middle_layer.add_module(\"ReLu\" + str(i),\n",
    "                                             torch.nn.ReLU())\n",
    "\n",
    "        self.output_layer = torch.nn.Sequential(\n",
    "            torch.nn.BatchNorm1d(hidden_dim),\n",
    "            torch.nn.Linear(hidden_dim, self.input_dim),\n",
    "            torch.nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "    def q_z(self, x):\n",
    "        \"\"\"\n",
    "        Generates distribution of z provided x.\n",
    "        Args:\n",
    "            x: Tensor - the matrix of shape batch_size x input_dim.\n",
    "        Returns:\n",
    "            tuple(Tensor, Tensor) - the normal distribution parameters.\n",
    "            mu: Tensor - the matrix of shape batch_size x latent_dim.\n",
    "            sigma: Tensor - the matrix of shape batch_size x latent_dim.\n",
    "        Example:\n",
    "            >>>\n",
    "        \"\"\"\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        proposal = self.proposal_z(x)\n",
    "        mu = self.proposal_mu(proposal)\n",
    "        sigma = torch.nn.Softplus()(self.proposal_sigma(proposal))\n",
    "        return mu, sigma\n",
    "\n",
    "    def p_z(self, num_samples):\n",
    "        \"\"\"\n",
    "        Generetes prior distribution of z.\n",
    "        Args:\n",
    "            num_samples: int - the number of samples.\n",
    "        Returns:\n",
    "            tuple(Tensor, Tensor) - the normal distribution parameters.\n",
    "                mu: Tensor - the matrix of shape num_samples x latent_dim.\n",
    "            \tsigma: Tensor - the matrix of shape num_samples x latent_dim.\n",
    "        Example:\n",
    "            >>>\n",
    "        \"\"\"\n",
    "        mu = torch.zeros([num_samples, self.latent_dim], device=self.device)\n",
    "        sigma = torch.ones([num_samples, self.latent_dim], device=self.device)\n",
    "        return mu, sigma\n",
    "\n",
    "    def sample_z(self, distr, num_samples=1):\n",
    "        \"\"\"\n",
    "        Generates samples from normal distribution q(z|x).\n",
    "        Args:\n",
    "            distr = (mu, sigma): tuple(Tensor, Tensor) - the normal distribution parameters.\n",
    "                mu: Tensor - the matrix of shape batch_size x latent_dim.\n",
    "                sigma: Tensor - the matrix of shape batch_size x latent_dim.\n",
    "            num_samples: int - the number of samples for each element.\n",
    "        Returns:\n",
    "            Tensor - the tensor of shape batch_size x num_samples x latent_dim - samples from normal distribution in latent space.\n",
    "        Example:\n",
    "            >>>\n",
    "        \"\"\"\n",
    "        mu, sigma = distr\n",
    "        mu = mu.to(self.device)\n",
    "        sigma = sigma.to(self.device)\n",
    "\n",
    "        batch_size = mu.shape[0]\n",
    "\n",
    "        bias = mu.view([batch_size, 1, self.latent_dim])\n",
    "\n",
    "        epsilon = torch.randn([batch_size, num_samples, self.latent_dim],\n",
    "                              requires_grad=True,\n",
    "                              device=self.device)\n",
    "        scale = sigma.view([batch_size, 1, self.latent_dim])\n",
    "\n",
    "        return bias + epsilon * scale\n",
    "\n",
    "    def sample_x(self, z):\n",
    "        z = z.to(self.device)\n",
    "\n",
    "        out = self.generative_layear(z.view([z.shape[0], self.latent_dim]))\n",
    "\n",
    "        if self.num_layers > 1:\n",
    "            out = self.middle_layer(out)\n",
    "\n",
    "        return self.output_layer(out)\n",
    "\n",
    "    def loss(self, batch_x, batch_y):\n",
    "        \"\"\"\n",
    "        Calculate ELBO approximation of log likelihood for given batch with negative sign.\n",
    "        Args:\n",
    "            batch_x: FloatTensor - the matrix of shape batch_size x input_dim.\n",
    "            batch_y: FloatTensor - dont uses parameter in this model.\n",
    "        Returns:\n",
    "            Tensor - scalar, ELBO approximation of log likelihood for given batch with negative sign.\n",
    "        Example:\n",
    "            >>>\n",
    "        \"\"\"\n",
    "        batch_x = batch_x.to(self.device)\n",
    "        batch_y = batch_y.to(self.device)\n",
    "\n",
    "        batch_size = batch_x.shape[0]\n",
    "\n",
    "        propos_distr = self.q_z(batch_x)\n",
    "        pri_distr = self.p_z(batch_size)\n",
    "\n",
    "        sample_x = self.sample_x(self.sample_z(propos_distr))\n",
    "\n",
    "        expectation = torch.nn.functional.mse_loss(sample_x, batch_x)\n",
    "        divergence = -1 * torch.mean(self.divergence_KL_normal(propos_distr, pri_distr), dim=0)\n",
    "\n",
    "        return expectation - divergence\n",
    "\n",
    "    @staticmethod\n",
    "    def divergence_KL_normal(q_distr, p_distr):\n",
    "        \"\"\"\n",
    "        Calculate KL-divergence KL(q||p) between n-pairs of normal distribution.\n",
    "        Args:\n",
    "            q_distr=(mu, sigma): tuple(Tensor, Tensor) - the normal distribution parameters.\n",
    "                mu: Tensor - the matrix of shape batch_size x latent_dim.\n",
    "                sigma: Tensor - the matrix of shape batch_size x latent_dim.\n",
    "            p_distr=(mu, sigma): tuple(Tensor, Tensor) - the normal distribution parameters.\n",
    "                mu: Tensor - the matrix of shape batch_size x latent_dim.\n",
    "                sigma: Tensor - the matrix of shape batch_size x latent_dim.\n",
    "        Returns:\n",
    "            Tensor - the vector of shape n, each value of which is a KL-divergence between pair of normal distribution.\n",
    "        Example:\n",
    "            >>>\n",
    "        \"\"\"\n",
    "        q_mu, q_sigma = q_distr\n",
    "        p_mu, p_sigma = p_distr\n",
    "\n",
    "        D_KL = torch.sum((q_sigma / p_sigma)**2, dim=1)\n",
    "        D_KL -= p_mu.shape[1]\n",
    "        D_KL += 2 * torch.sum(torch.log(p_sigma), dim=1) - \\\n",
    "            2 * torch.sum(torch.log(q_sigma), dim=1)\n",
    "        D_KL += torch.sum((p_mu - q_mu) * (p_mu - q_mu) / (p_sigma**2), dim=1)\n",
    "        return 0.5 * D_KL\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Generate decoded sample after encoding.\n",
    "        Args:\n",
    "            x: Tensor - the matrix of shape batch_size x input_dim.\n",
    "        Returns:\n",
    "            Tensor - the matrix of shape batch_size x input_dim.\n",
    "        Example:\n",
    "            >>>\n",
    "        \"\"\"\n",
    "\n",
    "        z = self.sample_z(self.q_z(x))\n",
    "\n",
    "        return self.sample_x(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выборка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx, :], 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def CreateDataset(input_size, train_size = 10000, test_size = 2000, n_clusters = 5):\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Параметры кластеров\n",
    "    means = np.random.rand(n_clusters, input_size) * 10\n",
    "    std_devs = np.random.rand(n_clusters, input_size)\n",
    "\n",
    "    # Генерация данных для каждого кластера\n",
    "    dataset_train = np.ndarray((train_size, input_size))\n",
    "    dataset_test = np.ndarray((test_size, input_size))\n",
    "\n",
    "    for i, (mean, std_dev) in enumerate(zip(means, std_devs)):\n",
    "        data = np.random.normal(loc=mean, scale=std_dev, size=(train_size, input_size))\n",
    "        dataset_train = np.concatenate((dataset_train, data), axis=0)\n",
    "\n",
    "        data = np.random.normal(loc=mean, scale=std_dev, size=(test_size, input_size))\n",
    "        dataset_test = np.concatenate((dataset_test, data), axis=0)\n",
    "\n",
    "    train_size *= n_clusters\n",
    "    test_size *= n_clusters\n",
    "\n",
    "    dataset_train = np.array(dataset_train, dtype=np.float32)\n",
    "    dataset_test = np.array(dataset_test, dtype=np.float32)\n",
    "\n",
    "    return dataset_train, dataset_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = ParameterGrid({'latent_dim': [50, 300],\n",
    "                      'hidden_dim': [100, 500],\n",
    "                      'num_layers': [2, 5],\n",
    "                      'input_dim':  [200, 400]})\n",
    "\n",
    "for item in tqdm(grid):\n",
    "    model = VAE(**item)\n",
    "    model.to(device)\n",
    "\n",
    "    name = 'experiment/latent{}_hidden{}_layers{}_input{}'.format(\n",
    "            item['latent_dim'], item['hidden_dim'], item['num_layers'], item['input_dim'])\n",
    "\n",
    "    writer = SummaryWriter(log_dir = name)\n",
    "\n",
    "    dataset_train, dataset_test = CreateDataset(item['input_dim'])\n",
    "\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()), lr = 1e-3)\n",
    "\n",
    "    call = callback(writer, Dataset(dataset_test), None, delimeter = 10)\n",
    "\n",
    "    trainer(count_of_epoch = 5,\n",
    "            batch_size = 64,\n",
    "            model = model,\n",
    "            dataset = Dataset(dataset_train),\n",
    "            loss_function = None,\n",
    "            optimizer = optimizer,\n",
    "            callback = call)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной работе был рассмотрен вариационной автокодировщик, на примере синтетической выборки из нормального распределения нескольких кластеров.\n",
    "\n",
    "Худшие результаты показали модели, у которых размерность латентного представления была больше скрытого. Такие модели также были неустойчевы к выбросам в выборке.\n",
    "\n",
    "Наилучшие результаты показали модели, у которых отношения размерности латентного представления к скрытому сильно меньше 1. Число слоев и входная размерность не сильно влияло.\n",
    "\n",
    "Повышение размерности скрытого представления повышало точность."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
