{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация аннотации к изображению"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предварительная работа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycocotools in /home/panterrich/.local/lib/python3.8/site-packages (2.0.7)\n",
      "Requirement already satisfied: numpy in /home/panterrich/.local/lib/python3.8/site-packages (from pycocotools) (1.23.1)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /home/panterrich/.local/lib/python3.8/site-packages (from pycocotools) (3.7.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/panterrich/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/panterrich/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools) (4.50.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/panterrich/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/panterrich/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/panterrich/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.5)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0; python_version < \"3.10\" in /home/panterrich/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools) (6.4.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/panterrich/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools) (10.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/panterrich/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools) (21.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/panterrich/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools) (1.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /home/panterrich/.local/lib/python3.8/site-packages (from importlib-resources>=3.2.0; python_version < \"3.10\"->matplotlib>=2.1.0->pycocotools) (3.8.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in /home/panterrich/.local/lib/python3.8/site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in /home/panterrich/.local/lib/python3.8/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /home/panterrich/.local/lib/python3.8/site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/panterrich/.local/lib/python3.8/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: click in /home/panterrich/.local/lib/python3.8/site-packages (from nltk) (8.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pycocotools\n",
    "%pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import math\n",
    "import nltk\n",
    "import os\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections             import Counter\n",
    "from copy                    import deepcopy\n",
    "from matplotlib.image        import imread\n",
    "from mpl_toolkits            import mplot3d\n",
    "from matplotlib              import gridspec\n",
    "from nerus                   import load_nerus\n",
    "from skimage.segmentation    import mark_boundaries\n",
    "from sklearn.metrics         import classification_report\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from torch.utils             import data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision             import datasets, transforms, models\n",
    "from tqdm.autonotebook       import tqdm\n",
    "from PIL                     import Image\n",
    "from pycocotools.coco        import COCO\n",
    "from urllib.request          import urlopen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/panterrich/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Установка вычислительного устройства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self, root, json, vocab, start, end, transform=None):\n",
    "        \"\"\"Set the path for images, captions and vocabulary wrapper.\n",
    "\n",
    "        Args:\n",
    "            root: image directory.\n",
    "            json: coco annotation file path.\n",
    "            vocab: vocabulary wrapper.\n",
    "            transform: image transformer.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.coco = COCO(json)\n",
    "        self.ids = list(self.coco.anns.keys())[start:end]\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (image and caption).\"\"\"\n",
    "        coco = self.coco\n",
    "        vocab = self.vocab\n",
    "        ann_id = self.ids[index]\n",
    "        caption = coco.anns[ann_id]['caption']\n",
    "        img_id = coco.anns[ann_id]['image_id']\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "        image = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert caption (string) to word ids.\n",
    "        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption = []\n",
    "        caption.append(vocab('<start>'))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        caption.append(vocab('<end>'))\n",
    "        target = torch.Tensor(caption)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "\n",
    "    We should build custom collate_fn rather than using default collate_fn,\n",
    "    because merging caption (including padding) is not supported in default.\n",
    "\n",
    "    Args:\n",
    "        data: list of tuple (image, caption).\n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "    return images, targets, lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(model, images, captions, lengths, optimizer, loss_function):\n",
    "    encoder, decoder = model\n",
    "\n",
    "    encoder.zero_grad()\n",
    "    decoder.zero_grad()\n",
    "\n",
    "    images = images.to(device)\n",
    "    captions = captions.to(device)\n",
    "\n",
    "    targets = torch.nn.utils.rnn.pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "    features = encoder(images)\n",
    "    outputs = decoder(features, captions, lengths)\n",
    "\n",
    "    loss = loss_function(outputs, targets)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.cpu().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_generator,\n",
    "                model,\n",
    "                loss_function,\n",
    "                optimizer,\n",
    "                callback = None):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    total = 0\n",
    "    for it, (images, captions, lengths) in enumerate(train_generator):\n",
    "        batch_loss = train_on_batch(model,\n",
    "                                    images,\n",
    "                                    captions,\n",
    "                                    lengths,\n",
    "                                    optimizer,\n",
    "                                    loss_function)\n",
    "        if callback is not None:\n",
    "            with torch.no_grad():\n",
    "                callback(model, batch_loss)\n",
    "\n",
    "        epoch_loss += batch_loss*len(images)\n",
    "        total += len(images)\n",
    "\n",
    "    return epoch_loss/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(count_of_epoch,\n",
    "            batch_size,\n",
    "            model,\n",
    "            dataset,\n",
    "            collate_fn,\n",
    "            loss_function,\n",
    "            optimizer,\n",
    "            callback = None):\n",
    "    iterations = tqdm(range(count_of_epoch), desc='epoch')\n",
    "    iterations.set_postfix({'train epoch loss': np.nan})\n",
    "\n",
    "    n_samples = len(dataset)\n",
    "    number_of_batch = n_samples//batch_size + (n_samples%batch_size>0)\n",
    "\n",
    "    for it in iterations:\n",
    "        batch_generator = tqdm(\n",
    "                torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            collate_fn=collate_fn),\n",
    "                leave=False, total=number_of_batch)\n",
    "\n",
    "        epoch_loss = train_epoch(\n",
    "            train_generator = batch_generator,\n",
    "            model = model,\n",
    "            loss_function = loss_function,\n",
    "            optimizer = optimizer,\n",
    "            callback = callback)\n",
    "\n",
    "        iterations.set_postfix({'train epoch loss': epoch_loss})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отслеживание обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 7772), started 0:08:45 ago. (Use '!kill 7772' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9eb55efa922f4f2d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9eb55efa922f4f2d\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir experiment/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_images(writer, dataset):\n",
    "    num = 10\n",
    "\n",
    "    images = []\n",
    "    for i in range(-num, 0):\n",
    "        image, _ = dataset.__getitem__(i)\n",
    "        images.append(image)\n",
    "\n",
    "    digit_size = (3, 256, 256)\n",
    "\n",
    "    fig = plt.figure(figsize=(36, 36 / 10 * (num // 10 + 1)))\n",
    "    gs = gridspec.GridSpec(1, num)\n",
    "    for i in range(num):\n",
    "        ax = fig.add_subplot(gs[i])\n",
    "        ax.imshow(images[i], interpolation='lanczos')\n",
    "        ax.axis('off')\n",
    "\n",
    "    writer.add_figure('VISUAL/images', fig, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class callback():\n",
    "    def __init__(self, writer, dataset, collate_fn, loss_function, vocab, delimeter = 100, batch_size=64):\n",
    "        self.step = 0\n",
    "        self.writer = writer\n",
    "        self.delimeter = delimeter\n",
    "        self.loss_function = loss_function\n",
    "        self.vocab = vocab\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.collate_fn = collate_fn\n",
    "\n",
    "    def forward(self, model, loss):\n",
    "        self.step += 1\n",
    "        self.writer.add_scalar('LOSS/train', loss, self.step)\n",
    "\n",
    "        if self.step % self.delimeter == 0:\n",
    "\n",
    "            batch_generator = torch.utils.data.DataLoader(dataset = self.dataset,\n",
    "                                                          batch_size = self.batch_size,\n",
    "                                                          collate_fn = self.collate_fn)\n",
    "            encoder, decoder = model\n",
    "\n",
    "            test_loss = 0\n",
    "            encoder.eval()\n",
    "            for it, (images, captions, lengths) in enumerate(batch_generator):\n",
    "                images = images.to(device)\n",
    "                captions = captions.to(device)\n",
    "\n",
    "                targets = torch.nn.utils.rnn.pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "                features = encoder(images)\n",
    "                outputs = decoder(features, captions, lengths)\n",
    "\n",
    "                test_loss += self.loss_function(outputs, targets).cpu().item()*len(images)\n",
    "\n",
    "            test_loss /= self.dataset.__len__()\n",
    "\n",
    "            self.writer.add_scalar('LOSS/test', test_loss, self.step)\n",
    "\n",
    "            num = 10\n",
    "\n",
    "            # Convert word_ids to words\n",
    "            for i in range(-num, 0):\n",
    "                sentence = ''\n",
    "                image = images[i].unsqueeze(0)\n",
    "\n",
    "                feature = encoder(image.to(device))\n",
    "                sampled_ids = decoder.sample(feature)\n",
    "                sampled_ids = sampled_ids[0].cpu().numpy() # (1, max_seq_length) -> (max_seq_length)\n",
    "\n",
    "                sampled_caption = []\n",
    "                for word_id in sampled_ids:\n",
    "                    word = self.vocab.idx2word[word_id]\n",
    "                    if word == '<end>':\n",
    "                        break\n",
    "                    if word == '<start>':\n",
    "                        continue\n",
    "\n",
    "                    sampled_caption.append(word)\n",
    "\n",
    "                sentence = ' '.join(sampled_caption)\n",
    "\n",
    "                self.writer.add_text('TEXT/test' + '[' + str(-i) + ']', sentence, self.step)\n",
    "\n",
    "    def __call__(self, model, loss):\n",
    "        return self.forward(model, loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(torch.nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
    "        self.resnet = torch.nn.Sequential(*modules)\n",
    "\n",
    "        self.last_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(resnet.fc.in_features, embed_size),\n",
    "            torch.nn.BatchNorm1d(embed_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        return self.last_layer(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(torch.nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.max_seg_length = max_seq_length\n",
    "\n",
    "        self.embed = torch.nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = torch.nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, features, captions, lengths):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embeddings, lengths, batch_first=True)\n",
    "\n",
    "        hiddens, _ = self.lstm(packed)\n",
    "\n",
    "        return self.linear(hiddens[0])\n",
    "\n",
    "    def sample(self, features, states=None):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "\n",
    "        for i in range(self.max_seg_length):\n",
    "            hiddens, states = self.lstm(inputs, states) # hiddens: (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear(hiddens.squeeze(1))   # outputs:  (batch_size, vocab_size)\n",
    "            _, predicted = outputs.max(1)               # predicted: (batch_size)\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embed(predicted)              # inputs: (batch_size, embed_size)\n",
    "            inputs = inputs.unsqueeze(1)                # inputs: (batch_size, 1, embed_size)\n",
    "\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)       # sampled_ids: (batch_size, max_seq_length)\n",
    "        return sampled_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выборка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def build_vocab(json, threshold):\n",
    "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
    "    coco = COCO(json)\n",
    "    counter = Counter()\n",
    "    ids = coco.anns.keys()\n",
    "    for i, id in enumerate(ids):\n",
    "        caption = str(coco.anns[id]['caption'])\n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # If the word frequency is less than 'threshold', then the word is discarded.\n",
    "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "\n",
    "    # Create a vocab wrapper and add some special tokens.\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<pad>')\n",
    "    vocab.add_word('<start>')\n",
    "    vocab.add_word('<end>')\n",
    "    vocab.add_word('<unk>')\n",
    "\n",
    "    # Add the words to the vocabulary.\n",
    "    for i, word in enumerate(words):\n",
    "        vocab.add_word(word)\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.44s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.37s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.39s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.37s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "root = 'data/resized2014'\n",
    "json = 'data/annotations_trainval2014/annotations/captions_train2014.json'\n",
    "\n",
    "train_size = 15000\n",
    "test_size = 3000\n",
    "threshold = 4\n",
    "\n",
    "vocab = build_vocab(json, threshold)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "dataset_train = CocoDataset(root, json, vocab, 0,          train_size,             transform)\n",
    "dataset_test  = CocoDataset(root, json, vocab, train_size, train_size + test_size, transform)\n",
    "\n",
    "images_test = CocoDataset(root, json, vocab, train_size, train_size + test_size, None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0049c2d5af94093912a7da469e80f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484debddcf5d4774b343e64b88df8061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf83a8e211645e2af30effcad04f981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98ee99a7529459ca58c6f4812dedfcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b8e5014f714812bb67b08a78bd408d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a3bcd6cc2e4601a037eb6f84ed5e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0fa619dd4f41d398e3e8882f5572a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44fdcc0ae104463a94ec0ed573035a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58d4ac55de14bbc96e6de8a2f583c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d627ff077ec43c7b165b91cfd92cba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c017aa0741e45008919778b360c2a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4355fc3690e74f2e8436ae1a0cb9813b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5472c9d5ed431991df83284b4a45af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3e0d94530046e1894ae0024e871bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd9a4ea840a4f8ca0144e0250c9e8ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64217bc705d94cfead54d0030bab2b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec50412649a49ab8ed96114fee2c28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5765f8b8ee843e8ac45c8be75fdf3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8b10ce2e1d4ee2815a15241bc8e686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615b092d6b7f4090badbd568211c1071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eadd9c0c1b6448bb7787674a3b871ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da191d72e3f4b0fbb8971e7c5473365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98478cc412c54671b2549f903f5207c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13143e1bf3794d3cacd48d0d9ff0b633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c2ec17b92842f1acefc8e9ee50ffc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b914ae1eea430fa97a4cb1170edbef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dcbf01fe9b2472996a970952ac1e19d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc96f914d8d45b69cdac8622f77a8c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d29077d31884b60b9ea44b173e9d781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198a93fd7c4e41c48c31b1f96d114b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8674cc1563e8483fb184922ebdcf1c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f96efa27d1034e4db71567fddbfe6389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a90c9e146814313b896a8921136b752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid = ParameterGrid({'emb_dim' : [300, 500],\n",
    "                      'hidden_dim': [600, 1000],\n",
    "                      'num_layers': [1, 3],\n",
    "                      'vocab_dim': [len(vocab)],\n",
    "                      'max_seq_length': [20]})\n",
    "\n",
    "for item in tqdm(grid):\n",
    "    encoder = EncoderCNN(item['emb_dim'])\n",
    "    decoder = DecoderRNN(item['emb_dim'],\n",
    "                         item['hidden_dim'],\n",
    "                         item['vocab_dim'],\n",
    "                         item['num_layers'],\n",
    "                         item['max_seq_length'])\n",
    "\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "\n",
    "    name = 'experiment/emb{}_hiden{}_layers{}_vocab{}'.format(\n",
    "            item['emb_dim'], item['hidden_dim'], item['num_layers'], item['vocab_dim'])\n",
    "\n",
    "    writer = SummaryWriter(log_dir = name)\n",
    "    add_images(writer, images_test)\n",
    "\n",
    "    params = list(decoder.parameters()) + list(encoder.last_layer.parameters())\n",
    "    optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "\n",
    "    loss_function = torch.nn.CrossEntropyLoss(ignore_index=vocab('<pad>'))\n",
    "\n",
    "    call = callback(writer, dataset_test, collate_fn, loss_function, vocab, delimeter = 10)\n",
    "\n",
    "    trainer(count_of_epoch = 3,\n",
    "            batch_size = 64,\n",
    "            model = (encoder, decoder),\n",
    "            dataset = dataset_train,\n",
    "            collate_fn = collate_fn,\n",
    "            loss_function = loss_function,\n",
    "            optimizer = optimizer,\n",
    "            callback = call)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученные результаты генераций получились достаточно посредственные, некоторые описания совпадают для картинок. Это может быть связано быть как и малостью выбранной подвыборки, так и некачественным подбором наборов параметров.\n",
    "\n",
    "Наихудшие модели получились с большим числом слоёв, модель получается сложной. Соотвественно лучше всего справились модели с 1 слоем. Остальные параметры (emb_dim, hidden_dim) почти не влияют на результаты.\n",
    "\n",
    "Размер словаря очень сильно вляет на качество генерации описаний. Если взять словарь только по описаниям тестовой выборки, то генерация заканчивалась с описанием составленных практически из одних артиклей."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
